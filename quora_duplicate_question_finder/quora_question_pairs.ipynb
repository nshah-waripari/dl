{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47e126cb062e588e7802cdfe8e23a92f327d1ee0"
      },
      "cell_type": "code",
      "source": "#download google embedding\nimport requests\n#url = 'https://s3.amazonaws.com/mordecai-geo/GoogleNews-vectors-negative300.bin.gz'\n#url = 'https://github.com/eyaler/word2vec-slim/blob/master/GoogleNews-vectors-negative300-SLIM.bin.gz?raw=true'\nurl = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\nr = requests.get(url, allow_redirects=True)\nopen('GoogleNews-vectors-negative300.bin.gz', 'wb').write(r.content)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a95408fd70e164c0c54091953bc2d41ec44489db"
      },
      "cell_type": "markdown",
      "source": "**Data Preprocessing**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b70bc62ad8ee8ef8591442958e58e7e1b7701317"
      },
      "cell_type": "code",
      "source": "#read the data as is via pandas dataframe\nimport re\nimport nltk\n\n\n#define some constants to be used later\nTRAIN_CSV = '../input/train.csv'\nTEST_CSV = '../input/test.csv'\nEMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n\n# Load training and test set\ntrain_df = pd.read_csv(TRAIN_CSV)#.sample(150000, random_state=0) #sample 50k for training set\ntest_df = pd.read_csv(TEST_CSV)#.sample(10000, random_state=0) #sample 5k for test set\ntest_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05ddb7da493c67a7813c0b95aad3ec675f406f2b"
      },
      "cell_type": "code",
      "source": "#data clean-up\nSTOP_WORDS = nltk.corpus.stopwords.words() # load stop words from nltk library\n\n\"\"\"\nremove chars that are not letters or numbers, lowercase, then remove stop words\nInput: sentence\nReturns: cleaned up sentence\n\"\"\"\ndef clean_sentence(val):\n    \n    regex = re.compile('([^\\s\\w]|_)+') # remove characters that are not letters or numbers\n    sentence = regex.sub('', val).lower() #lower case\n    sentence = sentence.split(\" \")\n    \n    #for word in list(sentence): #remove stop words\n    #    if word in STOP_WORDS:\n    #        sentence.remove(word)  \n            \n    sentence = \" \".join(sentence) \n    return sentence\n\"\"\"\ndrop nans, then apply 'clean_sentence' function to question1 and 2\nInput: dataframe\nReturns: dataframe with questions column cleaned-up\n\"\"\"\ndef clean_dataframe(data):\n    \n    data = data.dropna(how=\"any\")\n    \n    for col in ['question1', 'question2']:\n        data.loc[:,col] = data[col].apply(clean_sentence)\n    \n    return data\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b46920bb2016def1de9597cdd6ef9aa553e4cf5b",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train_clean_df = clean_dataframe(train_df)\n#test_clean_df = clean_dataframe(test_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c14859bdef9d990a4eb59b3ab7990d7c89cb69cf"
      },
      "cell_type": "code",
      "source": "train_clean_df.head(5)\n#train_clean_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfbd2f5edbd0cfcf7bb3cf22fdbf77de86e4b86a"
      },
      "cell_type": "code",
      "source": "#extract questions from the dataframe\nquestion1 = []\nquestion2 = []\nis_duplicate = []\nquestion1 = train_clean_df[\"question1\"].astype('str') \nquestion2 = train_clean_df[\"question2\"].astype('str') \nis_duplicate = train_clean_df[\"is_duplicate\"]\n\nprint (len(is_duplicate))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3dd0f6a31a9dfff190709f9cba0109766dfa0672"
      },
      "cell_type": "markdown",
      "source": "**Embedding Matrix**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98d4345821c80d43372a6a617ab1f11455ed06d5"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\n\nMAX_WORDS = 200000 #consider only top 200,000 words in the dataset\nMAX_SEQUENCE_LENGTH = 25\nEMBEDDING_DIM = 300\nquestions = question1 + question2\ntokenizer = Tokenizer(num_words=MAX_WORDS)\ntokenizer.fit_on_texts(questions)\nquestion1_word_sequences = tokenizer.texts_to_sequences(question1)\nquestion2_word_sequences = tokenizer.texts_to_sequences(question2)\nword_index = tokenizer.word_index\n\nprint(\"Words in index: %d\" % len(word_index))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "696969536f5ba0109461bf8bcb7fa42ef9408fa9"
      },
      "cell_type": "code",
      "source": "word_index['quora']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bde6386961dbea16e62bd8e4bc90f77592f9e77e"
      },
      "cell_type": "code",
      "source": "from gensim.models import word2vec, KeyedVectors\nembeddings_index = {}\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\nembedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i < MAX_WORDS:\n        if word in word2vec.vocab:\n            embedding_vector = word2vec.word_vec(word)\n            if embedding_vector is not None:          \n                embedding_matrix[i] = embedding_vector\n#del word2vec\nprint('Word embeddings: %d' % (word2vec.vocab['word'].count))\nprint('Embedding matrix shape: %s' % (str(embedding_matrix.shape)))\ndel word2vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2aae9437986eba839838aeb610d6956a496830e"
      },
      "cell_type": "markdown",
      "source": "**Prepare Train / Test data**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd696d1621fc34b4a2369daf6f8f7529f2ba86a9"
      },
      "cell_type": "code",
      "source": " from keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 50\nq1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nq2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = np.array(is_duplicate, dtype=int)\nprint('Shape of question1 data tensor:', q1_data.shape)\nprint('Shape of question2 data tensor:', q2_data.shape)\nprint('Shape of label tensor:', labels.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a13e5667adcd8918a4a5ba245983791caf1b80a"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX = np.stack((q1_data, q2_data), axis=1) #stack horizontally question 1 and question 2\ny = is_duplicate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\nQ1_train = X_train[:,0]\nQ2_train = X_train[:,1]\nQ1_test = X_test[:,0]\nQ2_test = X_test[:,1]\n\n# Convert labels to their numpy representations\ny_train = y_train.values\ny_test = y_test.values\n\n# Make sure everything is ok\nassert Q1_train.shape == Q2_train.shape\nassert len(X_train) == len(y_train)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f2c3ec03014904e90993a9015494e1f5bab4c71e"
      },
      "cell_type": "markdown",
      "source": "**Define Siamese Manhattan LSTM Model**\n\nReferences: <br>\nhttp://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf<br>\nhttps://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6384f7ab56b72cb4eb3c8d49f0ce228f937dc728"
      },
      "cell_type": "code",
      "source": "from keras.models import Model\nfrom keras.layers import Input, LSTM,TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.regularizers import l2\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.optimizers import Adadelta\n\n# Model variables\nn_hidden = 50\ngradient_clipping_norm = 1.25\nbatch_size = 64\nn_epoch = 15\n\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# The visible layer\nleft_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nright_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedding_layer = Embedding(len(embedding_matrix), EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n\n# Embedded version of the inputs\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\n# Since this is a siamese network, both sides share the same LSTM\nshared_lstm = LSTM(n_hidden)\n\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n\n# Calculates the distance as defined by the MaLSTM model\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n\n# Pack it all up into a model\nmalstm = Model([left_input, right_input], [malstm_distance])\n\n\nmalstm.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3df21f2a5f4080cfe7b00317b0150a4fb36e6004"
      },
      "cell_type": "markdown",
      "source": "**Run the model**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a126c0e3861588a55c54b604e01fe3f1523f9847",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from time import time\nimport datetime\n\nMODEL_WEIGHTS = 'quora_question_pairs_weights.h5'\ncallbacks = [ModelCheckpoint(MODEL_WEIGHTS, monitor='val_acc', save_best_only=True)]\n# Adadelta optimizer, with gradient clipping by norm\noptimizer = Adadelta(clipnorm=gradient_clipping_norm)\n\nmalstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n# Start training\ntraining_start_time = time()\n\nmalstm_trained = malstm.fit([Q1_train, Q2_train], y_train, batch_size=batch_size, epochs=n_epoch,\n                            validation_data=([Q1_test, Q2_test], y_test), callbacks = callbacks)\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "faa9986755693bc84d3f10a081ded3e736659f4b"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n# Plot accuracy\nplt.plot(malstm_trained.history['acc'])\nplt.plot(malstm_trained.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot loss\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e7e2e234aa19e53ac85f6ac61f8af00ab5944bf"
      },
      "cell_type": "code",
      "source": "model = malstm.load_weights(MODEL_WEIGHTS)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc9687cc57769bdb0edf0d4aab99d8d6f08cbf2a"
      },
      "cell_type": "code",
      "source": "test_df.tail(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7bd69a388c87e63c9ec97e06a06c25db9dd2ebd1"
      },
      "cell_type": "markdown",
      "source": "**Prepare Test Dataset**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8542224b620f5b040471244410c671a3645efdd8"
      },
      "cell_type": "code",
      "source": "#test_df.head(10)\n#x = test_df.loc[test_df['test_id'] == 2345793]\n#a =x['question1'].astype('str')\n#b =x['question2'].astype('str')\ntest_question1 = test_df[\"question1\"].astype('str') \ntest_question2 = test_df[\"question2\"].astype('str')\nt_question1_word_sequences = tokenizer.texts_to_sequences(test_question1)\nt_question2_word_sequences = tokenizer.texts_to_sequences(test_question2)\n\nt_q1_data = pad_sequences(t_question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nt_q2_data = pad_sequences(t_question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\npredictions = malstm.predict([t_q1_data, t_q2_data])\npredictions.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "569b60668a42fadf1da1cc883c190877da96530e"
      },
      "cell_type": "code",
      "source": "#flatten the predicted values into 1-d array\npred = predictions.flatten()\npred.shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2652e7081dfb7b280968f130d2beec0e43b663bd"
      },
      "cell_type": "markdown",
      "source": "**Create a submission file**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3bcf3223962a0e3e0b087c091d97c85e519305d"
      },
      "cell_type": "code",
      "source": "#submission = pd.DataFrame(predictions, columns=['is_duplicate'])\n#submission.insert(0, 'test_id', test.test_id)\nfile_name = 'submission_v2.csv'\npred_updated = np.where(pred > 0.5, 1, 0)\nsubmission = pd.DataFrame({'test_id': test_df['test_id'], 'is_duplicate': pred})\nsubmission.to_csv(file_name, index=False)\n\n#submission.head(10)\nsubmission.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "342a2fa56d729a1964ee55655f97a73b375cc8d0"
      },
      "cell_type": "code",
      "source": "submission.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9896a1f8e6110099edaeff59bba81fa9440b5e7c"
      },
      "cell_type": "code",
      "source": "#tests\n#x = test_df.loc[test_df['test_id'] == 4085]\n#x\n#a =x['question1'].astype('str')\n#b =x['question2'].astype('str')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae4315be47622a66440f549283cffeb2c4b45b20"
      },
      "cell_type": "code",
      "source": "#credit: https://www.kaggle.com/dansbecker/submitting-from-a-kernel/ (Dan B)\n# import the modules we'll need\n\"\"\"\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission_v1.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(submission)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ace6386dcd31cb63430d5cdd9172d3b011f77616"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}